{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Short description \n",
    "\n",
    "In the first part of the lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it:\n",
    "- starting from crawling documents, \n",
    "- then building an inverted index,\n",
    "- and answering queries using this index.\n",
    "\n",
    "Second part is devoted to spellchecking. You will implement some functions related to spellchecking.\n",
    "- K-gram index\n",
    "- weighted editorial distance\n",
    "- Norvig spellchecker\n",
    "\n",
    "\n",
    "# 1. [60] Building inverted index and answering queries\n",
    "\n",
    "## 1.1. [10] Preprocessing\n",
    "\n",
    "First, we need a unified approach to documents preprocessing. Implement a class responsible for that. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library or any other you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/olya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "                      'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with'}\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        #TODO word tokenize text using nltk lib\n",
    "        return word_tokenize(text) \n",
    "\n",
    "    \n",
    "    def stem(self, word, stemmer):\n",
    "        #TODO stem word using provided stemmer\n",
    "        return stemmer.stem(word)\n",
    "\n",
    "    \n",
    "    def is_apt_word(self, word):\n",
    "        #TODO check if word is appropriate - not a stop word and isalpha, \n",
    "        # i.e consists of letters, not punctuation, numbers, dates\n",
    "        return True if word not in self.stop_words and word.isalpha() else False\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        #TODO combine all previous methods together: tokenize lowercased text \n",
    "        # and stem it, ignoring not appropriate words\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stem(word.lower(),self.ps) for word in tokenized_text if self.is_apt_word(word.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. [35] Crawling and Indexing\n",
    "\n",
    "### 1.2.1. [10] Base classes\n",
    "\n",
    "Here are some base classes you will need for writing an indexer. The code from the first lab's solution is given, still you need to change some of it, namely, the `parse` function (it is also possible to use your own implementation from the first homework, but make sure that it is correct). The reason is it always makes complete parsing, which we want to avoid when we only need links, for example, or a specific portion of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                self.content = response.content\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def persist(self, path):\n",
    "        with open(os.path.join(path, quote(self.url).replace('/', '_')), 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def normalize(self, href):\n",
    "        if href is not None and href[:4] != 'http':\n",
    "            href = urllib.parse.urljoin(self.url, href)\n",
    "        return href\n",
    "\n",
    "    def parse(self, imgs = False, links = True, text=False):\n",
    "        #TODO change this method\n",
    "        \n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        model = BeautifulSoup(self.content, 'html.parser')\n",
    "        \n",
    "        self.anchors = []\n",
    "        self.images = []\n",
    "        self.text = []\n",
    "        \n",
    "        if links:\n",
    "            a = model.find_all('a')\n",
    "            for anchor in a:\n",
    "                href = self.normalize(anchor.get('href'))\n",
    "                text = anchor.text\n",
    "                self.anchors.append((text, href))\n",
    "                \n",
    "            i = model.findAll('iframe')\n",
    "            for iframe in i:\n",
    "                url = self.normalize(iframe.get('src'))\n",
    "                text = iframe.getText()\n",
    "                self.anchors.append((text, url))\n",
    "        \n",
    "        if imgs:\n",
    "            i = model.find_all('img')\n",
    "            for img in i:\n",
    "                href = self.normalize(img.get('src'))\n",
    "                self.images.append(href)\n",
    "        \n",
    "        if text:\n",
    "            texts = model.findAll(text=True)\n",
    "            visible_texts = filter(tag_visible, texts)  \n",
    "            self.text = u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. [20] Main class\n",
    "\n",
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk. \n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (of a form https://www.reuters.com/...) for visiting. To speed up, doesn't consider for visiting pages with content type other than html: '.pdf', '.mp3', '.avi', '.mp4', '.txt'. If encounters an article page (of a form https://www.reuters.com/article/...), saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`, `doc_id:url`\n",
    "    - `index`, `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    - `doc_lengths`, `doc_id:doc_length` \n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "Your crawler have to print visited urls as it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):      \n",
    "        # dictionaries to populate\n",
    "        self.doc_urls = {}        \n",
    "        self.index = {}\n",
    "        self.doc_lengths = {}\n",
    "        # preprocessor\n",
    "        self.prep = Preprocessor()\n",
    "        self.queue = Queue()\n",
    "        self.queued_urls = set()\n",
    "        self.visited_urls = set()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):        \n",
    "        #TODO generate url-s for visiting\n",
    "        self.limit = limit\n",
    "        self.create_queue(source,depth)\n",
    "        \n",
    "        if not os.path.exists(collection_path):\n",
    "                os.mkdir(collection_path)\n",
    "                \n",
    "        self.create_queue(source,depth)\n",
    "        print(self.queue.qsize())\n",
    "        #print(self.visited_urls)\n",
    "        \n",
    "        self.saved_articles = 0\n",
    "        while not self.queue.empty():\n",
    "            if limit is not None and self.saved_articles >= limit:\n",
    "                break\n",
    "            url = self.queue.get()\n",
    "            html_doc = HtmlDocument(url)\n",
    "            if html_doc.download():\n",
    "                no_error_flag = 0\n",
    "                try:\n",
    "                    html_doc.persist(collection_path)\n",
    "                    no_error_flag=1\n",
    "                except:\n",
    "                    print(\"could not persist \", url)\n",
    "                if no_error_flag==1:\n",
    "                    self.saved_articles+=1\n",
    "                    doc_id = len(self.doc_urls)+1\n",
    "                    self.doc_urls[doc_id]=url\n",
    "        for d_id in self.doc_urls.keys():\n",
    "            u = self.doc_urls[d_id]\n",
    "            doc = HtmlDocument(u)\n",
    "            doc.download()\n",
    "            self.index_doc(doc, d_id)\n",
    "            yield Document(u)\n",
    "        \n",
    "        # save results for later use\n",
    "        with open('doc_urls.p', 'wb') as fp:\n",
    "            pickle.dump(self.doc_urls, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('inverted_index.p', 'wb') as fp:\n",
    "            pickle.dump(self.index, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('doc_lengths.p', 'wb') as fp:\n",
    "            pickle.dump(self.doc_lengths, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def valid_url(self,url):\n",
    "        if url is None or '.mp3' in url or '.pdf' in url or '.avi' in url or '.mp4' in url or '.txt' in url:\n",
    "            return False\n",
    "        if 'https://www.reuters.com/' not in url:\n",
    "            return False\n",
    "        try:\n",
    "            resp = requests.get(url,allow_redirects=True,timeout=3)\n",
    "            if resp is not None and resp.status_code>=200 and resp.status_code < 300:\n",
    "                return True\n",
    "        except:\n",
    "            return False\n",
    "        return False\n",
    "        \n",
    "    def create_queue(self,source,depth):\n",
    "        print(source)\n",
    "        if depth==1:\n",
    "            if source not in self.queued_urls and self.valid_url(source) and 'https://www.reuters.com/article/' in source:\n",
    "                self.queued_urls.add(source)\n",
    "                self.queue.put(source)\n",
    "        else:\n",
    "            if source not in self.queued_urls and self.valid_url(source):\n",
    "                doc = HtmlDocument(source)\n",
    "                doc.download()\n",
    "                doc.parse(imgs=False,links=True,text=False)\n",
    "                if 'https://www.reuters.com/article/' in source:\n",
    "                    self.queue.put(source)\n",
    "                    print('Added to queue ', source)\n",
    "                for anchor in doc.anchors:\n",
    "                    _, url = anchor\n",
    "                    if url not in self.visited_urls:\n",
    "                        self.visited_urls.add(url)\n",
    "                        self.create_queue(url,depth-1)\n",
    "            \n",
    "        \n",
    "    def index_doc(self, doc, doc_id):\n",
    "        #TODO add documents to index \n",
    "        doc.parse(imgs=False,links=False,text=True)\n",
    "        terms = self.prep.preprocess(doc.text)\n",
    "        self.doc_lengths[doc_id]=len(terms)\n",
    "        for term in terms:\n",
    "            if term not in self.index.keys():\n",
    "                if self.limit is not None:\n",
    "                    self.index[term] = [0]+[[d_id, 0] for d_id in range(1,(min(len(self.doc_urls),self.limit)+1))]\n",
    "                else:\n",
    "                    self.index[term] = [0]+[[d_id, 0] for d_id in range(1,len(self.doc_urls)+1)]\n",
    "            self.index[term][0]+=1\n",
    "            self.index[term][doc_id] = list(self.index[term][doc_id])\n",
    "            self.index[term][doc_id][1]+=1\n",
    "            self.index[term][doc_id] = tuple(self.index[term][doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Tests\n",
    "\n",
    "Please make sure your crawler prints out urls with `print(k, c.url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "https://www.reuters.com/article/us-usa-semiconductors/intel-in-talks-to-produce-chips-for-automakers-within-six-to-nine-months-ceo-idUSKBN2BZ2C4\n",
      "https://www.reuters.com/article/us-gamestop-ceo-exclusive/exclusive-gamestop-initiates-search-for-new-ceo-sources-idUSKBN2BZ290\n",
      "https://www.reuters.com/article/us-usa-biden-chips/white-house-zeros-in-on-chip-shortage-in-meeting-with-company-officials-idUSKBN2BZ118\n",
      "https://www.reuters.com/article/us-nvidia-forecast/nvidia-expects-first-quarter-sales-to-exceed-5-3-billion-idUSKBN2BZ2CB\n",
      "https://www.reuters.com/article/us-nuance-commns-m-a-microsoft/microsoft-to-buy-ai-firm-nuance-for-16-billion-to-boost-healthcare-business-idUSKBN2BZ1FS\n",
      "1 https://www.reuters.com/article/us-usa-semiconductors/intel-in-talks-to-produce-chips-for-automakers-within-six-to-nine-months-ceo-idUSKBN2BZ2C4\n",
      "2 https://www.reuters.com/article/us-gamestop-ceo-exclusive/exclusive-gamestop-initiates-search-for-new-ceo-sources-idUSKBN2BZ290\n",
      "3 https://www.reuters.com/article/us-usa-biden-chips/white-house-zeros-in-on-chip-shortage-in-meeting-with-company-officials-idUSKBN2BZ118\n",
      "4 https://www.reuters.com/article/us-nvidia-forecast/nvidia-expects-first-quarter-sales-to-exceed-5-3-billion-idUSKBN2BZ2CB\n",
      "5 https://www.reuters.com/article/us-nuance-commns-m-a-microsoft/microsoft-to-buy-ai-firm-nuance-for-16-billion-to-boost-healthcare-business-idUSKBN2BZ1FS\n"
     ]
    }
   ],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/technology\", 2, \"test_collection\", 5):\n",
    "    print(k, c.url)\n",
    "    k += 1\n",
    "\n",
    "assert type(indexer.index) is dict\n",
    "assert type(indexer.index['reuter']) is list\n",
    "assert type(indexer.index['reuter'][0]) is int\n",
    "assert type(indexer.index['reuter'][1]) is tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Building an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "https://www.reuters.com/article/us-usa-minnesota-shooting/police-say-officer-mistakenly-fired-gun-instead-of-taser-in-killing-of-wright-idUSKBN2BZ21A\n",
      "https://www.reuters.com/article/us-usa-race-georgefloyd/a-big-mommas-boy-george-floyds-brother-recalls-childhood-at-chauvin-murder-trial-idUSKBN2BZ21M\n",
      "https://www.reuters.com/article/us-usa-police-virginia/virginia-police-officer-accused-of-assaulting-u-s-army-officer-fired-idUSKBN2BZ0HM\n",
      "https://www.reuters.com/article/us-shooting-knoxville/reports-of-shooting-at-knoxville-tennessee-high-school-with-multiple-victims-police-idUSKBN2BZ2L4\n",
      "https://www.reuters.com/article/us-usa-guns/exclusive-biden-plans-nominations-to-two-top-politically-sensitive-legal-posts-idUSKBN2BU2QP\n",
      "https://www.reuters.com/article/us-iran-nuclear-natanz/iran-blames-israel-for-natanz-nuclear-plant-outage-vows-revenge-idUSKBN2BZ0HD\n",
      "https://www.reuters.com/article/us-gamestop-ceo-exclusive/exclusive-gamestop-initiates-search-for-new-ceo-sources-idUSKBN2BZ290\n",
      "https://www.reuters.com/article/us-tech-antitrust-hawley/u-s-senator-wants-to-ban-big-tech-from-buying-anything-ever-again-idUSKBN2BZ2MD?il=0\n",
      "https://www.reuters.com/article/us-usa-stocks/wall-street-ends-lower-as-investors-await-earnings-inflation-data-idUSKBN2BZ16Y?il=0\n",
      "https://www.reuters.com/article/us-credit-suisse-gp-bonuses/credit-suisse-cuts-bonuses-following-archegos-loss-ft-idUSKBN2BZ2L8?il=0\n",
      "https://www.reuters.com/article/us-iran-nuclear-natanz/iran-blames-israel-for-natanz-nuclear-plant-outage-vows-revenge-idUSKBN2BZ0HD?il=0\n",
      "https://www.reuters.com/article/us-iran-nuclear-natanz-usa/u-s-denies-involvement-in-iran-nuclear-site-incident-idUSKBN2BZ25N?il=0\n",
      "https://www.reuters.com/article/us-finance-trading-bloomberg/brooklyn-man-pleads-not-guilty-to-using-bloomberg-reporters-information-for-insider-trading-idUSKBN2BZ2J1?il=0\n",
      "https://www.reuters.com/article/us-france-shooting/gunman-shoots-man-dead-injures-woman-in-front-of-paris-hospital-idUSKBN2BZ1FP?il=0\n",
      "https://www.reuters.com/article/us-airbus-management/shake-up-at-airbus-as-defence-and-technology-chiefs-quit-idUSKBN2BZ23J?il=0\n",
      "https://www.reuters.com/article/us-china-ant-group/china-extends-crackdown-on-jack-mas-empire-with-enforced-revamp-of-ant-group-idUSKBN2BZ134?il=0\n",
      "https://www.reuters.com/article/us-britain-greensill-cameron/uk-opens-probe-into-greensill-lobbying-by-ex-pm-cameron-idUSKBN2BZ184?il=0\n",
      "https://www.reuters.com/article/us-health-coronavirus-india-vaccine/india-approves-russias-sputnik-v-covid-19-vaccine-idUSKBN2BZ0ZQ?il=0\n",
      "https://www.reuters.com/article/us-usa-bonds-auctions/middling-u-s-treasury-auctions-mean-lull-in-yields-continues-for-now-idUSKBN2BZ2I4?il=0\n",
      "https://www.reuters.com/article/us-peru-election-race/socialists-vs-fujimori-peru-vote-sets-stage-for-polarized-presidential-run-off-idUSKBN2BZ1GZ?il=0\n",
      "https://www.reuters.com/article/us-global-forex/dollar-drops-as-traders-prepare-for-inflation-data-idUSKBN2BZ044?il=0\n",
      "https://www.reuters.com/article/us-health-coronavirus-canada/ontario-closes-in-person-schools-due-to-rising-covid-19-cases-premier-idUSKBN2BZ2GH?il=0\n",
      "https://www.reuters.com/article/us-kraft-heinz-mayochup/kraft-heinz-mayochup-dispute-revived-by-u-s-appeals-court-idUSKBN2BZ26J?il=0\n",
      "https://www.reuters.com/article/us-usa-biden-chips/white-house-zeros-in-on-chip-shortage-in-meeting-with-company-officials-idUSKBN2BZ118?il=0\n",
      "https://www.reuters.com/article/us-reuters-events-conference-autos/gm-marketing-spend-will-return-to-normal-levels-post-pandemic-idUSKBN2BZ1TU?il=0\n",
      "https://www.reuters.com/article/us-autos-tech-nvidia/nvidia-volvo-cars-accelerate-auto-industrys-data-processing-power-race-idUSKBN2BZ277?il=0\n",
      "https://www.reuters.com/article/us-nvidia-forecast/nvidia-expects-first-quarter-sales-to-exceed-5-3-billion-idUSKBN2BZ2CB?il=0\n",
      "https://www.reuters.com/article/us-health-coronavirus-canada-hospitals/ontario-hospitals-may-have-to-withhold-care-as-covid-19-fills-icus-idUSKBN2BZ205\n",
      "https://www.reuters.com/article/us-health-coronavirus-roche-regeneron-ph/regeneron-to-seek-u-s-ok-for-covid-19-cocktail-to-be-used-for-prevention-idUSKBN2BZ0CG\n",
      "https://www.reuters.com/article/us-health-coronavirus-india-vaccine/india-approves-russias-sputnik-v-covid-19-vaccine-idUSKBN2BZ0ZQ\n",
      "https://www.reuters.com/article/us-money-women-breadwinners-idUSKBN2BZ0BM\n",
      "https://www.reuters.com/article/us-myanmar-politics/myanmars-suu-kyi-asks-court-to-let-her-meet-lawyers-activists-urge-new-year-defiance-idUSKBN2BZ0KJ\n",
      "https://www.reuters.com/article/us-china-ant-group/china-extends-crackdown-on-jack-mas-empire-with-enforced-revamp-of-ant-group-idUSKBN2BZ134\n",
      "https://www.reuters.com/article/us-finance-trading-bloomberg/brooklyn-man-pleads-not-guilty-to-using-bloomberg-reporters-information-for-insider-trading-idUSKBN2BZ2J1\n",
      "https://www.reuters.com/article/us-airbus-management/shake-up-at-airbus-as-defence-and-technology-chiefs-quit-idUSKBN2BZ23J\n",
      "https://www.reuters.com/article/us-usa-stocks/wall-street-ends-lower-as-investors-await-earnings-inflation-data-idUSKBN2BZ16Y\n",
      "https://www.reuters.com/article/us-people-harvey-weinstein/harvey-weinstein-is-indicted-in-california-appears-at-extradition-hearing-idUSKBN2BZ2D3\n",
      "https://www.reuters.com/article/us-equal-pay-soccer/u-s-womens-national-team-to-appeal-pay-claims-after-working-conditions-settlement-idUSKBN2BZ2FQ\n",
      "https://www.reuters.com/article/us-tech-antitrust-hawley/u-s-senator-wants-to-ban-big-tech-from-buying-anything-ever-again-idUSKBN2BZ2MD\n",
      "https://www.reuters.com/article/us-credit-suisse-gp-bonuses/credit-suisse-cuts-bonuses-following-archegos-loss-ft-idUSKBN2BZ2L8\n",
      "https://www.reuters.com/article/us-usa-biden-chips/white-house-zeros-in-on-chip-shortage-in-meeting-with-company-officials-idUSKBN2BZ118\n",
      "https://www.reuters.com/article/us-global-markets/stocks-slip-from-record-peaks-before-earnings-reports-idUSKBN2BZ00Y\n",
      "https://www.reuters.com/article/us-usa-bonds-auctions/middling-u-s-treasury-auctions-mean-lull-in-yields-continues-for-now-idUSKBN2BZ2I4\n",
      "https://www.reuters.com/article/us-global-forex/dollar-drops-as-traders-prepare-for-inflation-data-idUSKBN2BZ044\n",
      "https://www.reuters.com/article/us-iran-nuclear-natanz-usa/u-s-denies-involvement-in-iran-nuclear-site-incident-idUSKBN2BZ25N\n",
      "https://www.reuters.com/article/us-france-shooting/gunman-shoots-man-dead-injures-woman-in-front-of-paris-hospital-idUSKBN2BZ1FP\n",
      "https://www.reuters.com/article/us-britain-greensill-cameron/uk-opens-probe-into-greensill-lobbying-by-ex-pm-cameron-idUSKBN2BZ184\n",
      "https://www.reuters.com/article/us-autos-tech-nvidia/nvidia-volvo-cars-accelerate-auto-industrys-data-processing-power-race-idUSKBN2BZ277\n",
      "https://www.reuters.com/article/us-nvidia-forecast/nvidia-expects-first-quarter-sales-to-exceed-5-3-billion-idUSKBN2BZ2CB\n",
      "https://www.reuters.com/article/us-usa-semiconductors/intel-in-talks-to-produce-chips-for-automakers-within-six-to-nine-months-ceo-idUSKBN2BZ2C4\n",
      "https://www.reuters.com/article/us-usa-diversity-diplomacy/u-s-state-department-names-former-ambassador-gina-abercrombie-winstanley-as-first-chief-diversity-officer-idUSKBN2BZ2AD\n",
      "https://www.reuters.com/article/us-usa-congress-infrastructure/biden-faces-growing-republican-skepticism-over-infrastructure-plan-idUSKBN2BZ10Q\n",
      "https://www.reuters.com/article/us-usa-biden-chips-manufacturing-factbox/factbox-biden-jobs-plan-includes-50-billion-for-chips-research-manufacturing-idUSKBN2BZ1YQ\n",
      "https://www.reuters.com/article/us-bmo-divestiture/bmo-to-sell-emea-asset-management-unit-for-870-million-to-focus-on-north-america-idUSKBN2BZ1HY\n",
      "https://www.reuters.com/article/us-europe-stocks/european-stocks-slip-from-record-highs-as-focus-shifts-to-earnings-idUSKBN2BZ0LR\n",
      "https://www.reuters.com/article/us-denmark-transportation-alstom/denmark-to-order-100-electric-trains-from-alstom-in-3-2-billion-deal-idUSKBN2BZ1O5\n",
      "1 https://www.reuters.com/article/us-usa-minnesota-shooting/police-say-officer-mistakenly-fired-gun-instead-of-taser-in-killing-of-wright-idUSKBN2BZ21A\n",
      "2 https://www.reuters.com/article/us-usa-race-georgefloyd/a-big-mommas-boy-george-floyds-brother-recalls-childhood-at-chauvin-murder-trial-idUSKBN2BZ21M\n",
      "3 https://www.reuters.com/article/us-usa-police-virginia/virginia-police-officer-accused-of-assaulting-u-s-army-officer-fired-idUSKBN2BZ0HM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 https://www.reuters.com/article/us-shooting-knoxville/reports-of-shooting-at-knoxville-tennessee-high-school-with-multiple-victims-police-idUSKBN2BZ2L4\n",
      "5 https://www.reuters.com/article/us-usa-guns/exclusive-biden-plans-nominations-to-two-top-politically-sensitive-legal-posts-idUSKBN2BU2QP\n",
      "6 https://www.reuters.com/article/us-iran-nuclear-natanz/iran-blames-israel-for-natanz-nuclear-plant-outage-vows-revenge-idUSKBN2BZ0HD\n",
      "7 https://www.reuters.com/article/us-gamestop-ceo-exclusive/exclusive-gamestop-initiates-search-for-new-ceo-sources-idUSKBN2BZ290\n",
      "8 https://www.reuters.com/article/us-tech-antitrust-hawley/u-s-senator-wants-to-ban-big-tech-from-buying-anything-ever-again-idUSKBN2BZ2MD?il=0\n",
      "9 https://www.reuters.com/article/us-usa-stocks/wall-street-ends-lower-as-investors-await-earnings-inflation-data-idUSKBN2BZ16Y?il=0\n",
      "10 https://www.reuters.com/article/us-credit-suisse-gp-bonuses/credit-suisse-cuts-bonuses-following-archegos-loss-ft-idUSKBN2BZ2L8?il=0\n",
      "11 https://www.reuters.com/article/us-iran-nuclear-natanz/iran-blames-israel-for-natanz-nuclear-plant-outage-vows-revenge-idUSKBN2BZ0HD?il=0\n",
      "12 https://www.reuters.com/article/us-iran-nuclear-natanz-usa/u-s-denies-involvement-in-iran-nuclear-site-incident-idUSKBN2BZ25N?il=0\n",
      "13 https://www.reuters.com/article/us-finance-trading-bloomberg/brooklyn-man-pleads-not-guilty-to-using-bloomberg-reporters-information-for-insider-trading-idUSKBN2BZ2J1?il=0\n",
      "14 https://www.reuters.com/article/us-france-shooting/gunman-shoots-man-dead-injures-woman-in-front-of-paris-hospital-idUSKBN2BZ1FP?il=0\n",
      "15 https://www.reuters.com/article/us-airbus-management/shake-up-at-airbus-as-defence-and-technology-chiefs-quit-idUSKBN2BZ23J?il=0\n",
      "16 https://www.reuters.com/article/us-china-ant-group/china-extends-crackdown-on-jack-mas-empire-with-enforced-revamp-of-ant-group-idUSKBN2BZ134?il=0\n",
      "17 https://www.reuters.com/article/us-britain-greensill-cameron/uk-opens-probe-into-greensill-lobbying-by-ex-pm-cameron-idUSKBN2BZ184?il=0\n",
      "18 https://www.reuters.com/article/us-health-coronavirus-india-vaccine/india-approves-russias-sputnik-v-covid-19-vaccine-idUSKBN2BZ0ZQ?il=0\n",
      "19 https://www.reuters.com/article/us-usa-bonds-auctions/middling-u-s-treasury-auctions-mean-lull-in-yields-continues-for-now-idUSKBN2BZ2I4?il=0\n",
      "20 https://www.reuters.com/article/us-peru-election-race/socialists-vs-fujimori-peru-vote-sets-stage-for-polarized-presidential-run-off-idUSKBN2BZ1GZ?il=0\n",
      "21 https://www.reuters.com/article/us-global-forex/dollar-drops-as-traders-prepare-for-inflation-data-idUSKBN2BZ044?il=0\n",
      "22 https://www.reuters.com/article/us-health-coronavirus-canada/ontario-closes-in-person-schools-due-to-rising-covid-19-cases-premier-idUSKBN2BZ2GH?il=0\n",
      "23 https://www.reuters.com/article/us-kraft-heinz-mayochup/kraft-heinz-mayochup-dispute-revived-by-u-s-appeals-court-idUSKBN2BZ26J?il=0\n",
      "24 https://www.reuters.com/article/us-usa-biden-chips/white-house-zeros-in-on-chip-shortage-in-meeting-with-company-officials-idUSKBN2BZ118?il=0\n",
      "25 https://www.reuters.com/article/us-reuters-events-conference-autos/gm-marketing-spend-will-return-to-normal-levels-post-pandemic-idUSKBN2BZ1TU?il=0\n",
      "26 https://www.reuters.com/article/us-autos-tech-nvidia/nvidia-volvo-cars-accelerate-auto-industrys-data-processing-power-race-idUSKBN2BZ277?il=0\n",
      "27 https://www.reuters.com/article/us-nvidia-forecast/nvidia-expects-first-quarter-sales-to-exceed-5-3-billion-idUSKBN2BZ2CB?il=0\n",
      "28 https://www.reuters.com/article/us-health-coronavirus-canada-hospitals/ontario-hospitals-may-have-to-withhold-care-as-covid-19-fills-icus-idUSKBN2BZ205\n",
      "29 https://www.reuters.com/article/us-health-coronavirus-roche-regeneron-ph/regeneron-to-seek-u-s-ok-for-covid-19-cocktail-to-be-used-for-prevention-idUSKBN2BZ0CG\n",
      "30 https://www.reuters.com/article/us-health-coronavirus-india-vaccine/india-approves-russias-sputnik-v-covid-19-vaccine-idUSKBN2BZ0ZQ\n",
      "31 https://www.reuters.com/article/us-money-women-breadwinners-idUSKBN2BZ0BM\n",
      "32 https://www.reuters.com/article/us-myanmar-politics/myanmars-suu-kyi-asks-court-to-let-her-meet-lawyers-activists-urge-new-year-defiance-idUSKBN2BZ0KJ\n",
      "33 https://www.reuters.com/article/us-china-ant-group/china-extends-crackdown-on-jack-mas-empire-with-enforced-revamp-of-ant-group-idUSKBN2BZ134\n",
      "34 https://www.reuters.com/article/us-finance-trading-bloomberg/brooklyn-man-pleads-not-guilty-to-using-bloomberg-reporters-information-for-insider-trading-idUSKBN2BZ2J1\n",
      "35 https://www.reuters.com/article/us-airbus-management/shake-up-at-airbus-as-defence-and-technology-chiefs-quit-idUSKBN2BZ23J\n",
      "36 https://www.reuters.com/article/us-usa-stocks/wall-street-ends-lower-as-investors-await-earnings-inflation-data-idUSKBN2BZ16Y\n",
      "37 https://www.reuters.com/article/us-people-harvey-weinstein/harvey-weinstein-is-indicted-in-california-appears-at-extradition-hearing-idUSKBN2BZ2D3\n",
      "38 https://www.reuters.com/article/us-equal-pay-soccer/u-s-womens-national-team-to-appeal-pay-claims-after-working-conditions-settlement-idUSKBN2BZ2FQ\n",
      "39 https://www.reuters.com/article/us-tech-antitrust-hawley/u-s-senator-wants-to-ban-big-tech-from-buying-anything-ever-again-idUSKBN2BZ2MD\n",
      "40 https://www.reuters.com/article/us-credit-suisse-gp-bonuses/credit-suisse-cuts-bonuses-following-archegos-loss-ft-idUSKBN2BZ2L8\n",
      "41 https://www.reuters.com/article/us-usa-biden-chips/white-house-zeros-in-on-chip-shortage-in-meeting-with-company-officials-idUSKBN2BZ118\n",
      "42 https://www.reuters.com/article/us-global-markets/stocks-slip-from-record-peaks-before-earnings-reports-idUSKBN2BZ00Y\n",
      "43 https://www.reuters.com/article/us-usa-bonds-auctions/middling-u-s-treasury-auctions-mean-lull-in-yields-continues-for-now-idUSKBN2BZ2I4\n",
      "44 https://www.reuters.com/article/us-global-forex/dollar-drops-as-traders-prepare-for-inflation-data-idUSKBN2BZ044\n",
      "45 https://www.reuters.com/article/us-iran-nuclear-natanz-usa/u-s-denies-involvement-in-iran-nuclear-site-incident-idUSKBN2BZ25N\n",
      "46 https://www.reuters.com/article/us-france-shooting/gunman-shoots-man-dead-injures-woman-in-front-of-paris-hospital-idUSKBN2BZ1FP\n",
      "47 https://www.reuters.com/article/us-britain-greensill-cameron/uk-opens-probe-into-greensill-lobbying-by-ex-pm-cameron-idUSKBN2BZ184\n",
      "48 https://www.reuters.com/article/us-autos-tech-nvidia/nvidia-volvo-cars-accelerate-auto-industrys-data-processing-power-race-idUSKBN2BZ277\n",
      "49 https://www.reuters.com/article/us-nvidia-forecast/nvidia-expects-first-quarter-sales-to-exceed-5-3-billion-idUSKBN2BZ2CB\n",
      "50 https://www.reuters.com/article/us-usa-semiconductors/intel-in-talks-to-produce-chips-for-automakers-within-six-to-nine-months-ceo-idUSKBN2BZ2C4\n",
      "51 https://www.reuters.com/article/us-usa-diversity-diplomacy/u-s-state-department-names-former-ambassador-gina-abercrombie-winstanley-as-first-chief-diversity-officer-idUSKBN2BZ2AD\n",
      "52 https://www.reuters.com/article/us-usa-congress-infrastructure/biden-faces-growing-republican-skepticism-over-infrastructure-plan-idUSKBN2BZ10Q\n",
      "53 https://www.reuters.com/article/us-usa-biden-chips-manufacturing-factbox/factbox-biden-jobs-plan-includes-50-billion-for-chips-research-manufacturing-idUSKBN2BZ1YQ\n",
      "54 https://www.reuters.com/article/us-bmo-divestiture/bmo-to-sell-emea-asset-management-unit-for-870-million-to-focus-on-north-america-idUSKBN2BZ1HY\n",
      "55 https://www.reuters.com/article/us-europe-stocks/european-stocks-slip-from-record-highs-as-focus-shifts-to-earnings-idUSKBN2BZ0LR\n",
      "56 https://www.reuters.com/article/us-denmark-transportation-alstom/denmark-to-order-100-electric-trains-from-alstom-in-3-2-billion-deal-idUSKBN2BZ1O5\n"
     ]
    }
   ],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/\", 3, \"docs_collection\"):\n",
    "    print(k, c.url)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5. [5] Index statistics\n",
    "\n",
    "Load an index and print the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index length 3042\n",
      "\n",
      "Top terms by number of documents they apperared in:\n",
      "[('zurich', 57), ('zionist', 57), ('zieminski', 57), ('zhu', 57), ('zhongguancun', 57), ('zhejiang', 57), ('zero', 57), ('zarif', 57), ('zammit', 57), ('zainab', 57), ('yuan', 57), ('yourself', 57), ('your', 57), ('younger', 57), ('young', 57), ('you', 57), ('york', 57), ('yingzhi', 57), ('yield', 57), ('yet', 57)]\n",
      "\n",
      "Top terms by overall frequency:\n",
      "[('s', 312), ('reuter', 264), ('said', 263), ('not', 155), ('all', 133), ('report', 119), ('delay', 115), ('advertis', 113), ('thomson', 112), ('use', 98), ('have', 92), ('new', 90), ('monday', 87), ('compani', 87), ('after', 86), ('billion', 80), ('our', 79), ('support', 77), ('we', 76), ('see', 76)]\n"
     ]
    }
   ],
   "source": [
    "# load index, doc_lengths and doc_urls\n",
    "with open('inverted_index.p', 'rb') as fp:\n",
    "    index = pickle.load(fp)\n",
    "with open('doc_lengths.p', 'rb') as fp:\n",
    "    doc_lengths = pickle.load(fp)\n",
    "with open('doc_urls.p', 'rb') as fp:\n",
    "    doc_urls = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "print('Total index length', len(index))\n",
    "print('\\nTop terms by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1])) for i in range(20)])\n",
    "print('\\nTop terms by overall frequency:')\n",
    "sorted_by_freq = sorted(index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [15] Answering a query (finally)\n",
    "\n",
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class QueryProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        prep = Preprocessor()\n",
    "        # pre-process query the same way as documents\n",
    "        query = prep.preprocess(raw_query)\n",
    "        # count frequency\n",
    "        return Counter(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        #TODO retrieve a set of documents containing all query terms\n",
    "        results = set([index[list(query)[0]][doc_id][0] for doc_id in range(1,len(index[list(query)[0]])) if index[list(query)[0]][doc_id][1]>0])\n",
    "        for key in list(query)[1:]:\n",
    "            results = results&set([index[key][doc_id][0] for doc_id in range(1,len(index[key])) if index[key][doc_id][1]>0])\n",
    "        return results\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "        #TODO retrieve relevant documents with scores\n",
    "        docs_num = len(doc_lengths)\n",
    "        avgdl = sum(doc_lengths.values())/docs_num\n",
    "        results = {}\n",
    "        for term in list(query):\n",
    "            for pair in index[term][1:]:\n",
    "                doc_id = pair[0]\n",
    "                if doc_id not in results.keys():\n",
    "                    results[doc_id]=0\n",
    "                docs_num_with_term = len(index[term])-1\n",
    "                idf = math.log((docs_num-docs_num_with_term+0.5)/(docs_num_with_term+0.5)+1)\n",
    "                calc = (pair[1]*(k1+1))/(pair[1]+k1*(1-b+(b*doc_lengths[doc_id]/avgdl)))\n",
    "                results[doc_id]+=idf*calc\n",
    "        return results\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4:20, 5:30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4,2)]}\n",
    "\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "assert all(k in okapi_res for k in (1, 2, 3))\n",
    "assert not any(k in okapi_res for k in (4, 5))\n",
    "assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [40] Sorri not veri gud in inglish\n",
    "\n",
    "Have you ever googled someone's name without knowing exactly how it should be written? Were you ever reluctant to look up the correct spelling of a query you typed? Or just unable to type properly because of being in a rush? Modern search engines usually do a pretty good job in deciphering defective user input. In order to be able to do that, a good spell-checking mechanism should be incorporated into a search procedure. Today we will take one step further towards building a good search engine and work on tolerant retrieval with respect to user queries. We will consider two cases:\n",
    "\n",
    "1. Users know that they don't know the correct spelling OR they want to get the results that follow some known pattern, so the use so called wildcards - queries like `retr*val`;\n",
    "2. Users don't know the correct spelling OR they don't care OR they are in a rush OR they expect that mistakes will be corrected OR /your option/... so they make mistakes and we need to handle them using:\n",
    "\n",
    "    2.1. Trigrams with Jaccard coefficient;\n",
    "    \n",
    "    2.2. Simple spellchecker by Peter Norvig with QWERTY weights;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. [25] Handling wildcards\n",
    "\n",
    "We will handle wildcard queries using k-grams. K-gram is a list of consecutive k chars in a string - i.e., for the word *'star'*, it will be '*\\$st*', '*sta*', '*tar*', and '*ar$*', if we take k=3. Take a look at [book](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf) *chapter 3.2.2* to understand how k-grams can help efficiently match a wildcard against dictionary words. Here we will only consider wildcards with star symbols (may be multiple).\n",
    "\n",
    "Notice that for building k-grams index, **we will need a vocabulary of original word forms** to compare words in user input to the vocabulary of \"correct\" words (think why inverted index which we built for stemmed words doesn't work here).   \n",
    "\n",
    "You need to implement the following:\n",
    "\n",
    "- `build_inverted_index_orig_forms` - creates inverted index of original world forms from `facts` list, which is already given to you.  \n",
    "    Output format: `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    \n",
    "\n",
    "- `build_k_gram_index` - creates k-gram index which maps every k-gram encountered in facts collection to a list of words containing this k-gram. Use the abovementioned inverted index of original words to construct this index.  \n",
    "    Output format: `'k_gram': ['word1_with_k_gram', 'word2_with_k_gram', ...]`\n",
    "    \n",
    "    \n",
    "- `generate_wildcard_options` - produce a list of vocabulary words matching given wildcard by intersecting postings of k-grams present in the wildcard (refer to *ch 3.2.2*). \n",
    "\n",
    "- `search_wildcard` - return list of facts that contain the words matching a wildcard query.\n",
    "\n",
    "\n",
    "We will use the dataset with curious facts for testing.\n",
    "\n",
    "### 2.1.1. Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151. Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
      "152. There are more stars in space than there are grains of sand on every beach in the world.\n",
      "153. For every human on Earth there are 1.6 million ants.\n",
      "154. The total weight of all those ants, however, is about the same as all the humans.\n",
      "155. On Jupiter and Saturn it rains diamonds.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "data_url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "local_filename, headers = urllib.request.urlretrieve(data_url)\n",
    "\n",
    "facts = []\n",
    "with open(local_filename) as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        facts.append(line.strip('\\n'))\n",
    "        \n",
    "print(*facts[-5:], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. [25] Implementation of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import ngrams\n",
    "\n",
    "def build_inverted_index_orig_forms(documents):\n",
    "    #TODO build an inverted index of original word forms \n",
    "    # (without stemming, just word tokenized and lowercased) \n",
    "    prep = Preprocessor()\n",
    "    inverted_index = {}\n",
    "    for doc_id in range(len(documents)):\n",
    "        terms = [word.lower() for word in prep.tokenize(documents[doc_id]) if prep.is_apt_word(word)]\n",
    "        #print(terms)\n",
    "        for term in terms:\n",
    "            if term not in inverted_index.keys():\n",
    "                inverted_index[term] = [0]+[[d_id, 0] for d_id in range(1,len(documents)+1)]\n",
    "            inverted_index[term][0]+=1\n",
    "            #print(inverted_index[term], doc_id+1)\n",
    "            inverted_index[term][doc_id+1] = list(inverted_index[term][doc_id+1])\n",
    "            inverted_index[term][doc_id+1][1]+=1\n",
    "            inverted_index[term][doc_id+1] = tuple(inverted_index[term][doc_id+1])\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def build_k_gram_index(inverted_index, k):\n",
    "    #TODO build index of k-grams for dictionary words. \n",
    "    # Padd with '$' ($word$) before splitting to k-grams    \n",
    "    k_gram_index = {}\n",
    "    for word in list(inverted_index):\n",
    "        word_for_kgram = '$'+word+'$'\n",
    "        k_grams = ngrams(word_for_kgram,k)\n",
    "        for kgram in k_grams:\n",
    "            kgram = \"\".join(kgram)\n",
    "            if kgram not in k_gram_index.keys():\n",
    "                k_gram_index[kgram] = []\n",
    "            if word not in k_gram_index[kgram]:\n",
    "                k_gram_index[kgram].append(word)\n",
    "    return k_gram_index\n",
    "\n",
    "\n",
    "def generate_wildcard_options(wildcard, k_gram_index, inverted_index):\n",
    "    #TODO for a given wildcard return all words matching it using k-grams\n",
    "    # refer to book chapter 3.2.2\n",
    "    # don't forget to pad wildcard with '$', when appropriate    \n",
    "    parts = str('$'+wildcard+'$').split('*')\n",
    "    k = len(list(k_gram_index)[0])\n",
    "    results = set()\n",
    "    for part in parts:\n",
    "        k_grams_for_search = set()\n",
    "        l=0\n",
    "        r=l+k\n",
    "        while l in range(0,len(part)+1) and r in range(0,len(part)+1):\n",
    "            k_grams_for_search.add(part[l:r])\n",
    "            l+=1\n",
    "            r=l+k\n",
    "        for kgram in k_grams_for_search:\n",
    "            if len(results)==0:\n",
    "                results = set(k_gram_index[kgram])\n",
    "            else:\n",
    "                results = results&set(k_gram_index[kgram])\n",
    "    return [result for result in results if re.match(wildcard.replace('*','.*'), result)]\n",
    "\n",
    "\n",
    "def search_wildcard(wildcard, k_gram_index, index, docs):\n",
    "    #TODO retrive list of documnets (facts) that contain words matching wildcard\n",
    "    suggestions = generate_wildcard_options(wildcard, k_gram_index, index)\n",
    "    results = []\n",
    "    for word in suggestions:\n",
    "        results = results+ [docs[pair[0]-1] for pair in index[word][1:] if pair[1] > 0]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recorded', 'received', 'reduced']\n",
      "4. The largest \u001b[1m\u001b[91mrecorded\u001b[0m snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
      "102. More than 50% of the people in the world have never made or \u001b[1m\u001b[91mreceived\u001b[0m a telephone call.\n",
      "134. A person can live without food for about a month, but only about a week without water. If the amount of water in your body is \u001b[1m\u001b[91mreduced\u001b[0m by just 1%, you'll feel thirsty. If it's \u001b[1m\u001b[91mreduced\u001b[0m by 10%, you'll die.\n"
     ]
    }
   ],
   "source": [
    "index_orig_forms = build_inverted_index_orig_forms(facts)\n",
    "k_gram_index = build_k_gram_index(index_orig_forms, 3)\n",
    "\n",
    "wildcard = \"re*ed\"\n",
    "\n",
    "wildcard_options = generate_wildcard_options(wildcard, k_gram_index, index_orig_forms)\n",
    "print(wildcard_options)\n",
    "assert(len(wildcard_options) >= 3)\n",
    "assert(\"red\" not in wildcard_options) \n",
    "\n",
    "wildcard_results = search_wildcard(wildcard, k_gram_index, index_orig_forms, facts)\n",
    "# some pretty printing\n",
    "for r in wildcard_results:\n",
    "    # highlight terms for visual evaluation\n",
    "    for term in wildcard_options:\n",
    "        r = re.sub(r'(' + term + ')', r'\\033[1m\\033[91m\\1\\033[0m', r, flags=re.I)\n",
    "    print(r)\n",
    "\n",
    "assert(len(wildcard_results) >=3)\n",
    "\n",
    "assert \"13. James Buchanan, the 15th U.S. president continuously bought slaves with his own money in order to free them.\" in search_wildcard(\"pres*dent\", k_gram_index, index_orig_forms, facts)\n",
    "assert \"40. 9 out of 10 Americans are deficient in Potassium.\" in search_wildcard(\"p*tas*um\", k_gram_index, index_orig_forms, facts)\n",
    "assert \"61. A man from Britain changed his name to Tim Pppppppppprice to make it harder for telemarketers to pronounce.\" in search_wildcard(\"*price\", k_gram_index, index_orig_forms, facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. [15] Handling typos\n",
    "\n",
    "### 2.2.0. Dataset \n",
    "\n",
    "Download github typo dataset from [here](https://github.com/mhagiwara/github-typo-corpus).\n",
    "Load it with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jsonlines in /home/olya/.local/lib/python3.6/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 245909\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
    "\n",
    "dataset = []\n",
    "other_langs = set()\n",
    "\n",
    "with jsonlines.open(dataset_file) as reader:\n",
    "    for obj in reader:\n",
    "        for edit in obj['edits']:\n",
    "            if edit['src']['lang'] != 'eng':\n",
    "                other_langs.add(edit['src']['lang'])\n",
    "                continue\n",
    "\n",
    "            if edit['is_typo']:\n",
    "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
    "                if src.lower() != tgt.lower():\n",
    "                    dataset.append((edit['src']['text'], edit['tgt']['text']))\n",
    "                \n",
    "print(f\"Dataset size = {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore sample typos\n",
    "Please, explore the dataset. You may see, that this is\n",
    "- mostly markdown\n",
    "- some common mistakes with do/does\n",
    "- some just refer to punctuation typos (which we do not consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \"\"\"Make am instance. =>         \"\"\"Make an instance.\n",
      "* travis: test agains Node.js 11 => * travis: test against Node.js 11\n",
      "The parser receive a string and returns an array inside a user-provided  => The parser receives a string and returns an array inside a user-provided \n",
      "CSV data is send through the `write` function and the resulted data is obtained => CSV data is sent through the `write` function and the resulting data is obtained\n",
      "One useful function part of the Stream API is `pipe` to interact between  => One useful function of the Stream API is `pipe` to interact between \n",
      "source to a `stream.Writable` object destination. This example available as  => source to a `stream.Writable` object destination. This example is available as \n",
      "`node samples/pipe.js` read the file, parse its content and transform it. => `node samples/pipe.js` and reads the file, parses its content and transforms it.\n",
      "Most of the generator is imported from its parent project [CSV][csv] in a effort  => Most of the generator is imported from its parent project [CSV][csv] in an effort \n",
      "*   `quote`             Optionnal character surrounding a field, one character only, defaults to double quotes.    => *   `quote`             Optional character surrounding a field, one character only, defaults to double quotes.   \n",
      "The parser receive a string and return an array inside a user-provided  => The parser receive a string and returns an array inside a user-provided \n"
     ]
    }
   ],
   "source": [
    "for pair in dataset[1010:1020]:\n",
    "    print(f\"{pair[0]} => {pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.0.1. Build a dataset vocabulary\n",
    "Here we prepare a vocabulary for spellchecker testing and for estimating overall correction quality. Consider only word-level. Be carefull, there is markdown (e.g. \\`name\\`. \\[url\\]\\(http://url)) and comment symbols (\\#, //, \\*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sent):\n",
    "    # splits sentence to words, filtering out non-alphabetical terms\n",
    "    words = nltk.word_tokenize(sent)    \n",
    "    words_filtered = filter(lambda x: x.isalpha(), words)\n",
    "    return words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63724"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = Counter()\n",
    "for pair in dataset:\n",
    "    for word in sent_to_words(pair[1].lower()):\n",
    "        vocabulary[word] += 1\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('function', 6193), ('de', 82), ('deutsch', 4), ('nocomments', 2), ('you', 42075), ('can', 26027), ('disable', 532), ('comments', 360), ('for', 44756), ('the', 207017)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "print(list(islice(vocabulary.items(), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. [15] Implement context-independent spellcheker (Trigrams with Jaccard coefficient) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_typo_kgram(word, k_gram_index) -> list:\n",
    "    #TODO return best matches with respect to Jaccard index   \n",
    "    k = len(list(k_gram_index)[0])\n",
    "    word_for_kgram = '$'+word+'$'\n",
    "    word_kgrams = ngrams(word_for_kgram,k)\n",
    "    \n",
    "    results = []\n",
    "    suggestions = []\n",
    "    for kgram in word_kgrams:\n",
    "        if (\"\".join(kgram)) in k_gram_index.keys():\n",
    "            suggestions = suggestions + k_gram_index[\"\".join(kgram)]\n",
    "    suggestions = Counter(suggestions)\n",
    "    for suggestion in list(suggestions):\n",
    "        results.append((suggestion, jaccard_coefficient(word, suggestion, suggestions, k)))\n",
    "    results.sort(key = lambda x: x[1], reverse = True)\n",
    "    return [result[0] for result in results[:3]]\n",
    "\n",
    "def jaccard_coefficient(word, suggestion, suggestions, k):\n",
    "    jc = suggestions[suggestion]/((len(word)+2-k+1)+(len(suggestion)+2-k+1)-suggestions[suggestion])\n",
    "    return jc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enough', 'eno', 'enought']\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "\n",
    "k_gram_index_github = build_k_gram_index(vocabulary, 3)\n",
    "print(fix_typo_kgram(\"enouh\", k_gram_index_github)[:20])\n",
    "assert \"enough\" in fix_typo_kgram(\"enouh\", k_gram_index_github), \"Assert k-gram failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. [Bonuses]\n",
    "\n",
    "### 2.3.1. [Bonus] QWERTY - Editorial distance\n",
    "\n",
    "Write code to compute weighted QWERTY-editorial distance.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/KB_United_States.svg/640px-KB_United_States.svg.png\" width=\"640\"/> \n",
    "\n",
    "Use this image to prepare weight function:\n",
    "- all letter pairs which share vertical border will get 0.5 multiplier **for replace** (`df`, `cv`, `ui`, ...)\n",
    "- all letter pairs which share at least some horizontal border will get 0.7 multiplier **for replace** (`dc`, `dr`, `km`, ...)\n",
    "- other operations are not scaled (x1 multiplier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neighbors(keyboard):\n",
    "    result = []\n",
    "    for l in range(len(keyboard)-1):\n",
    "        result.append(keyboard[l]+keyboard[l+1])\n",
    "        result.append(keyboard[l+1]+keyboard[l])\n",
    "    return result\n",
    "        \n",
    "vertical_neighbors = set()\n",
    "horizontal_neighbors = set()\n",
    "\n",
    "for s in ['qwertyuiop[]\\\\', \"asdfghjkl;'\",'zxcvbnm,./','1234567890-=']:\n",
    "    horizontal_neighbors = horizontal_neighbors.union(set(generate_neighbors(s)))\n",
    "          \n",
    "for s in ['1qaz','2q','3w','2w','3e','4e','4r','5r','5t','6t','6y',\n",
    "          '7y','7u','8u','8i','9i','9o','0o','0p','-p','-[','=[','=]',\n",
    "         '2wsx','3edc','4rfv','5tgb','6yhn','7ujm','8ik,','9ol.','0p;/',\"-['']\",\n",
    "         'wa','es','rd','tf','yg','uh','ij','ok','pl','[;',\"]'\",\n",
    "         'sz','dx','fc','gv','hb','jn','km','l,',';.',\"'/\"]:\n",
    "    vertical_neighbors = vertical_neighbors.union(set(generate_neighbors(s)))\n",
    "\n",
    "def replace_weight(let1, let2) -> float:\n",
    "    # TODO what is the weight for a pair of letters being replaced? Note this function should be commutative\n",
    "    if str(let1+let2) in horizontal_neighbors:\n",
    "          return 0.5\n",
    "    if str(let1+let2) in vertical_neighbors:\n",
    "        return 0.7\n",
    "    return 1\n",
    "\n",
    "def qwerty_edit_dist(s1, s2) -> float:\n",
    "    # TODO compute the Damerau-Levenshtein distance between two given strings (s1 and s2)\n",
    "          \n",
    "    #SOURCE: https://www.guyrutenberg.com/2008/12/15/damerau-levenshtein-distance-in-python/ \n",
    "    d = {}\n",
    "    lenstr1 = len(s1)\n",
    "    lenstr2 = len(s2)\n",
    "    for i in range(-1,lenstr1+1):\n",
    "        d[(i,-1)] = i+1\n",
    "    for j in range(-1,lenstr2+1):\n",
    "        d[(-1,j)] = j+1\n",
    "\n",
    "    for i in range(lenstr1):\n",
    "        for j in range(lenstr2):\n",
    "            if s1[i] == s2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = replace_weight(s1[i],s2[j])\n",
    "            d[(i,j)] = min(\n",
    "                           d[(i-1,j)] + 1, # deletion\n",
    "                           d[(i,j-1)] + 1, # insertion\n",
    "                           d[(i-1,j-1)] + cost, # substitution\n",
    "                          )\n",
    "            if i and j and s1[i]==s2[j-1] and s1[i-1] == s2[j]:\n",
    "                d[(i,j)] = min (d[(i,j)], d[i-2,j-2] + cost) # transposition\n",
    "\n",
    "    return d[lenstr1-1,lenstr2-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "assert qwerty_edit_dist(\"korrectud\", \"corrected\") == 2.0, \"Edit distance is computed incorrectly\"\n",
    "assert qwerty_edit_dist(\"soem\", \"some\") == 1.0, \"Edit distance is computed incorrectly\"\n",
    "assert qwerty_edit_dist(\"one\", \"one\") == 0.0, \"Edit distance is computed incorrectly\"\n",
    "assert qwerty_edit_dist(\"fihure\", \"figure\") == 0.5, \"Edit distance is computed incorrectly\"\n",
    "assert qwerty_edit_dist(\"fivure\", \"figure\") == 0.7, \"Edit distance is computed incorrectly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. [Bonus] Norvig's spellchecker with QWERTY weights\n",
    "\n",
    "You can base your code on [Norvig's corrector](https://norvig.com/spell-correct.html), but you should be sure you account the fact, that typos for close letters cost less. This should be considered in ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOURCE: https://norvig.com/spell-correct.html\n",
    "import requests\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, candidate, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    p = WORDS[candidate]\n",
    "    #print(word, candidate, p, qwerty_edit_dist(word, candidate))\n",
    "    return bool(p)*(1/(1+qwerty_edit_dist(word, candidate)))\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) | known(edits1(word)) | known(edits2(word)) | set([word]))\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def fix_typo_qwerty_norvig(word) -> str:\n",
    "    #TODO return best matching result for the word\n",
    "    results = [(candidate,P(word,candidate)) for candidate in candidates(word)]\n",
    "    results = sorted(results, key = lambda x: x[1], reverse = True)\n",
    "#     print(known(['confidence']))\n",
    "#     print(candidates(word))\n",
    "#     print(results)\n",
    "    return results[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "assert fix_typo_qwerty_norvig(\"korrectud\") == \"corrected\", \"Norvig's correcter doesn't work\"\n",
    "assert fix_typo_qwerty_norvig(\"speling\") == \"spelling\", \"Norvig's correcter doesn't work\"\n",
    "assert fix_typo_qwerty_norvig(\"condidence\") == \"confidence\", \"Norvig's correcter doesn't work\"\n",
    "assert fix_typo_qwerty_norvig(\"fpx\") == \"fox\", \"Norvig's correcter doesn't work\"\n",
    "assert fix_typo_qwerty_norvig(\"fux\") == \"fix\", \"Norvig's correcter doesn't work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Estimate quality of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Norvig accuracy (1258) = 0.1258\n",
      "k-gram accuracy (1719) = 0.1719\n"
     ]
    }
   ],
   "source": [
    "norvig, kgram = 0, 0\n",
    "limit = 10000\n",
    "counter = limit\n",
    "for i, (src, target) in enumerate(dataset):\n",
    "    if i == limit:\n",
    "        break\n",
    "    words = sent_to_words(src.lower())\n",
    "    # word suspected for typos\n",
    "    sn, sk = src.lower(), src.lower()\n",
    "    for word in words:\n",
    "        if word not in vocabulary and word.isalpha():\n",
    "            # top-1 accuracy\n",
    "            try:\n",
    "                wn, wk = fix_typo_qwerty_norvig(word), \\\n",
    "                         fix_typo_kgram(word, k_gram_index_github)[0]\n",
    "            except:\n",
    "                print(word)\n",
    "            sn = sn.replace(word, wn)\n",
    "            sk = sk.replace(word, wk)\n",
    "    norvig += int(sn == target.lower())\n",
    "    kgram += int(sk == target.lower())\n",
    "\n",
    "print(f\"Norvig accuracy ({norvig}) = {norvig / limit}\")\n",
    "print(f\"k-gram accuracy ({kgram}) = {kgram / limit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
