# -*- coding: utf-8 -*-
"""2021S - Vector Space Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167yvlUCVVMSsUND_fnNN0YvUl_UOZds8

# Words, concepts and search

You are given a collection of texts -- let's call it dataset. Each string is a separate document. For each test we will answer exactly 2 questions:
- Which of the documents (`doc_id`) is the closest to a given query?
- How many "concepts" (`concept_count`) are enough to represent this dataset?

Thus your result (answer) will consist of 2 integers, separated by a space. Like this: `doc_id concept_count`.

## Let's consider the test example:

`input.txt`

```
c d b.
d e a.
a b c.
a b c d.
d c a b.
a c.      # <--- the last one is the query
```

## Let's do vectorization
Reuse this in your solutions
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# inp = """c d b.
# d e a.
# a b c.
# a b c d.
# d c a b.
# a c.""".split('\n')

with open('input.txt', 'r') as f:
  inp = f.read().split('\n')

dataset, query = inp[:-1], inp[-1]

vect = TfidfVectorizer(
            analyzer='word',
            stop_words=None,
            token_pattern=r"(?u)\b\w+\b"    # (?u)\b\w\w+\b -- default pattern: (?u) -- unicode modifier, \b -- word border, \w\w+ = 2+ letters
)
DTM = vect.fit_transform(dataset).todense()
# print("Vocabulary:", vect.get_feature_names())
# print(DTM)

# print("\nIs it normed?\n")
# print(DTM @ DTM.T)

"""### So, we are ready to answer question 1. 
Which of the documents is the closest to a given query?
"""

from numpy.linalg import svd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
query_vector = vect.transform([query]).todense()
cosines_raw = cosine_similarity(DTM, query_vector)# ... oops. Your code here. Compute COSINE SIMILARITIES (1 is match, 0 - orthogonal)
# print("Cosine similarities of query and dataset:\n", cosines_raw)
max_doc = np.argmax(cosines_raw)
# print(max_doc)
# ... oops. Some more code here. Find an id of the best matching document (string number in test data)

"""### Time for question 2.
How many concepts are enough to express our dataset?

In other words, how many orthogonal components do we need to pass `allclose` test?

**NB: Can you just take the data and run PCA? Will it change the cosine metric?**

Implement reduced SVD decomposition to pass the test.
"""

from numpy.linalg import svd

U, sigma, Vh = np.linalg.svd(DTM, full_matrices=True)
Sigma = np.diag(sigma)
k=0
for i in range(1, min(DTM.shape[1], DTM.shape[0]) + 1):
    doc_embeddings = np.dot(U[:,:i],Sigma[:i,:i])
    projection = Vh[:i][:]
    DTM_approx = np.dot(doc_embeddings,projection)# ...
    # print(f"If we take {i} components, allclose =", np.allclose(DTM, DTM_approx, atol=0.01))
    if np.allclose(DTM, DTM_approx, atol=0.01):
        k=i
        break

#k = 4

doc_embeddings =  np.dot(U[:,:k],Sigma[:k,:k]) 
projection =      Vh[:k][:]
query_embedding = projection @ query_vector.T

cosines = doc_embeddings @ query_embedding
# print(f"Cosine similarities of query and dataset (after reduction to {k} dimensions):\n", cosines)
# assert np.allclose(cosines, cosines_raw), "Cosine similariries are not close"

with open('output.txt','w+') as fout:
  fout.write(str(max_doc)+' '+str(k))

"""Exactly the same! We verified the job!

### And the answer is...

Thus, looking at the result of previous block, we can state:
    
`output.txt`
```
2 4
```
"""